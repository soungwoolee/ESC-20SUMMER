{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESC Summer 1주차 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7조 곽현지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1) 이 경우에는 type1 error가 높을까요? type2 error가 높을까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 경우 threshold가 0.3으로 낮기 때문에, FP는 높고 FN은 낮다. FN은 type1 error, FP는 type2 error에 대응하기 떄문에 treshold가 낮은 경우애는 type2 erro가 type1 error보다 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2-1) Accuracy, Precision and Recall이  무엇인지 TP, FP, FN, TN의 식으로 나타내 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = (TP + TN) / (TP + FP + FN + TN)  :맞게 예측한 비율  \n",
    "Precision = TP / (TP + FP)  :모델이 TRUE로 예측한 것 중, 실제 TRUE의 비율  \n",
    "Recall = TP / (TP + TN)  :실제 TRUE인 것 중, 모델이 TRUE로 예측한 비율  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2-2) Precision and Recall에 관한 예시를 들어주세요. 생각한 예시에서 threshold를 높인다는 것은 무슨 의미인가요? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "은행이 고객의 대출가능여부를 판단할때에는 threshold를 높이는 것이 합리적이다. 은행의 입장에서 고객이 대출금을 상환하지 않는 것은 영업적 손해에 해당하기 때문에 이를 방지하기 위해서는 threshold를 높여 대출심사를 까다롭게 수행하는 것이 힙리적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3) 코드 따라해보고 주석 달기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "train_dataset = dsets.MNIST(root ='./data',  \n",
    "                            train = True,  #Train = True면 trainset\n",
    "                            transform = transforms.ToTensor(), \n",
    "                            download = True) \n",
    "  \n",
    "test_dataset = dsets.MNIST(root ='./data',  \n",
    "                           train = False,  \n",
    "                           transform = transforms.ToTensor()) \n",
    "\n",
    "# Hyper Parameters  \n",
    "input_size = 784  #28*28\n",
    "num_classes = 10  #0~9\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Dataset Loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model\n",
    "class LogisticRegression(nn.Module): \n",
    "    def __init__(self, input_size, num_classes): \n",
    "        super(LogisticRegression, self).__init__() \n",
    "        self.linear = nn.Linear(input_size, num_classes) \n",
    "  \n",
    "    def forward(self, x): \n",
    "        out = self.linear(x) \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_size, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function(classification problem -> crossentrophy)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "# optimizer(SGD)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/ 5], Step: [ 100/ 600], Loss: 2.2367\n",
      "Epoch: [ 1/ 5], Step: [ 200/ 600], Loss: 2.1012\n",
      "Epoch: [ 1/ 5], Step: [ 300/ 600], Loss: 2.0236\n",
      "Epoch: [ 1/ 5], Step: [ 400/ 600], Loss: 1.9473\n",
      "Epoch: [ 1/ 5], Step: [ 500/ 600], Loss: 1.8521\n",
      "Epoch: [ 1/ 5], Step: [ 600/ 600], Loss: 1.8009\n",
      "Epoch: [ 2/ 5], Step: [ 100/ 600], Loss: 1.7530\n",
      "Epoch: [ 2/ 5], Step: [ 200/ 600], Loss: 1.6610\n",
      "Epoch: [ 2/ 5], Step: [ 300/ 600], Loss: 1.5721\n",
      "Epoch: [ 2/ 5], Step: [ 400/ 600], Loss: 1.5947\n",
      "Epoch: [ 2/ 5], Step: [ 500/ 600], Loss: 1.5055\n",
      "Epoch: [ 2/ 5], Step: [ 600/ 600], Loss: 1.4754\n",
      "Epoch: [ 3/ 5], Step: [ 100/ 600], Loss: 1.3811\n",
      "Epoch: [ 3/ 5], Step: [ 200/ 600], Loss: 1.4604\n",
      "Epoch: [ 3/ 5], Step: [ 300/ 600], Loss: 1.4158\n",
      "Epoch: [ 3/ 5], Step: [ 400/ 600], Loss: 1.2610\n",
      "Epoch: [ 3/ 5], Step: [ 500/ 600], Loss: 1.2618\n",
      "Epoch: [ 3/ 5], Step: [ 600/ 600], Loss: 1.2896\n",
      "Epoch: [ 4/ 5], Step: [ 100/ 600], Loss: 1.2621\n",
      "Epoch: [ 4/ 5], Step: [ 200/ 600], Loss: 1.1452\n",
      "Epoch: [ 4/ 5], Step: [ 300/ 600], Loss: 1.2470\n",
      "Epoch: [ 4/ 5], Step: [ 400/ 600], Loss: 1.0941\n",
      "Epoch: [ 4/ 5], Step: [ 500/ 600], Loss: 1.1330\n",
      "Epoch: [ 4/ 5], Step: [ 600/ 600], Loss: 1.0809\n",
      "Epoch: [ 5/ 5], Step: [ 100/ 600], Loss: 1.0418\n",
      "Epoch: [ 5/ 5], Step: [ 200/ 600], Loss: 1.0984\n",
      "Epoch: [ 5/ 5], Step: [ 300/ 600], Loss: 1.1325\n",
      "Epoch: [ 5/ 5], Step: [ 400/ 600], Loss: 1.0802\n",
      "Epoch: [ 5/ 5], Step: [ 500/ 600], Loss: 1.0213\n",
      "Epoch: [ 5/ 5], Step: [ 600/ 600], Loss: 0.9504\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad() # reset all gradients to 0\n",
    "        outputs = model(images) # forward propagation\n",
    "        loss = criterion(outputs, labels) # calculate the loss\n",
    "        loss.backward() # backward propagation\n",
    "        optimizer.step() # updating weights\n",
    "        \n",
    "        if (i + 1) % 100 == 0: \n",
    "            print('Epoch: [% d/% d], Step: [% d/% d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.data.item()))\n",
    "        \n",
    "# loss.data[0] 작동 안됨. loss.data(tensor형태) or loss.data.item()(상수형태) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images:  83 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model \n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader: \n",
    "    images = Variable(images.view(-1, 28 * 28)) \n",
    "    outputs = model(images) \n",
    "    _, predicted = torch.max(outputs.data, 1) # 주어진 텐서 배열의 최대값이 들어있는 index 리턴\n",
    "    total += labels.size(0) \n",
    "    correct += (predicted == labels).sum() \n",
    "  \n",
    "print('Accuracy of the model on the 10000 test images: % d %%' % (100* correct/total)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4-1) 2-4 코드에서 optim.SDG를 사용하지 않고 코드를 짠다면 어떤 방식으로 짜야할까요? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training을 위한 for문에, weight(bias)를 update하는 w.data = w.data - learningrate * w.grad.data와  \n",
    "초기화하는 w.grad.data.zero_() 구문을 추가한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.895, b: 0.945 Cost: 0.621575\n",
      "Epoch  100/1000 W: 0.834, b: 0.394 Cost: 0.016864\n",
      "Epoch  200/1000 W: 0.922, b: 0.186 Cost: 0.003754\n",
      "Epoch  300/1000 W: 0.963, b: 0.088 Cost: 0.000836\n",
      "Epoch  400/1000 W: 0.983, b: 0.041 Cost: 0.000186\n",
      "Epoch  500/1000 W: 0.992, b: 0.020 Cost: 0.000041\n",
      "Epoch  600/1000 W: 0.996, b: 0.009 Cost: 0.000009\n",
      "Epoch  700/1000 W: 0.998, b: 0.004 Cost: 0.000002\n",
      "Epoch  800/1000 W: 0.999, b: 0.002 Cost: 0.000000\n",
      "Epoch  900/1000 W: 1.000, b: 0.001 Cost: 0.000000\n",
      "Epoch 1000/1000 W: 1.000, b: 0.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "\n",
    "# initiaize weights\n",
    "w = Variable(torch.Tensor([1.0]), requires_grad = True)\n",
    "b = Variable(torch.Tensor([1.0]), requires_grad = True)\n",
    "\n",
    "def forward(x):\n",
    "    return x * w + b\n",
    "\n",
    "def cost(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "# training data\n",
    "nb_epochs = 1000 \n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for x_val, y_val in zip(x_train, y_train):\n",
    "        c = cost(x_val, y_val)\n",
    "        c.backward()\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "        b.data = b.data - 0.01 * b.grad.data\n",
    "        \n",
    "        # weight update 후 gradient 초기화 \n",
    "        w.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, w.item(), b.item(), c.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4-2) Gradient descent 코드 구현하는 문제입니다. 2-4 코드에서 다른 optimizer 3개를 이용하여 결과값을 비교해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2152bd27590>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.010, b: 0.010 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.660, b: 0.628 Cost: 0.081239\n",
      "Epoch  200/1000 W: 0.733, b: 0.591 Cost: 0.050898\n",
      "Epoch  300/1000 W: 0.774, b: 0.501 Cost: 0.036588\n",
      "Epoch  400/1000 W: 0.815, b: 0.410 Cost: 0.024475\n",
      "Epoch  500/1000 W: 0.854, b: 0.324 Cost: 0.015306\n",
      "Epoch  600/1000 W: 0.888, b: 0.248 Cost: 0.008968\n",
      "Epoch  700/1000 W: 0.917, b: 0.184 Cost: 0.004927\n",
      "Epoch  800/1000 W: 0.941, b: 0.132 Cost: 0.002537\n",
      "Epoch  900/1000 W: 0.959, b: 0.091 Cost: 0.001224\n",
      "Epoch 1000/1000 W: 0.972, b: 0.061 Cost: 0.000552\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True) \n",
    "b = torch.zeros(1, requires_grad=True) \n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.Adam([W, b], lr=0.01) \n",
    "\n",
    "nb_epochs = 1000 \n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ASGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.093, b: 0.040 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.873, b: 0.289 Cost: 0.012043\n",
      "Epoch  200/1000 W: 0.900, b: 0.227 Cost: 0.007443\n",
      "Epoch  300/1000 W: 0.921, b: 0.179 Cost: 0.004601\n",
      "Epoch  400/1000 W: 0.938, b: 0.140 Cost: 0.002844\n",
      "Epoch  500/1000 W: 0.951, b: 0.110 Cost: 0.001758\n",
      "Epoch  600/1000 W: 0.962, b: 0.087 Cost: 0.001087\n",
      "Epoch  700/1000 W: 0.970, b: 0.068 Cost: 0.000673\n",
      "Epoch  800/1000 W: 0.976, b: 0.054 Cost: 0.000416\n",
      "Epoch  900/1000 W: 0.981, b: 0.042 Cost: 0.000258\n",
      "Epoch 1000/1000 W: 0.985, b: 0.033 Cost: 0.000160\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True) \n",
    "b = torch.zeros(1, requires_grad=True) \n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.ASGD([W, b], lr=0.01) \n",
    "\n",
    "nb_epochs = 1000 \n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.000, b: 0.000 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.004, b: 0.004 Cost: 4.612791\n",
      "Epoch  200/1000 W: 0.009, b: 0.009 Cost: 4.545029\n",
      "Epoch  300/1000 W: 0.015, b: 0.015 Cost: 4.466577\n",
      "Epoch  400/1000 W: 0.022, b: 0.022 Cost: 4.379364\n",
      "Epoch  500/1000 W: 0.029, b: 0.029 Cost: 4.284824\n",
      "Epoch  600/1000 W: 0.037, b: 0.037 Cost: 4.184107\n",
      "Epoch  700/1000 W: 0.046, b: 0.046 Cost: 4.078178\n",
      "Epoch  800/1000 W: 0.055, b: 0.055 Cost: 3.967873\n",
      "Epoch  900/1000 W: 0.064, b: 0.064 Cost: 3.853932\n",
      "Epoch 1000/1000 W: 0.074, b: 0.074 Cost: 3.737018\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "y_train = torch.FloatTensor([[1], [2], [3]]) \n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True) \n",
    "b = torch.zeros(1, requires_grad=True) \n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.Adadelta([W, b], lr=0.01) \n",
    "\n",
    "nb_epochs = 1000 \n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() \n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
